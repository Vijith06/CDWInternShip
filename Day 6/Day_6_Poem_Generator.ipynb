{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vijith06/CDWInternShip/blob/day6/Day_6_Poem_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python Poem generator"
      ],
      "metadata": {
        "id": "RMy_G0EBA62W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoemGenerator:\n",
        "    def __init__(self):\n",
        "        self.greeting = \"I am a poem generator assistant, expert in generating poem in Shakespearean terms. Please ask me a poem related query.\"\n",
        "\n",
        "    def generate_poem(self, topic, lines=4):\n",
        "        if topic:\n",
        "            poem = \"\"\n",
        "            for i in range(lines):\n",
        "                if i % 2 == 0:\n",
        "                    poem += f\"Oh, {topic}! Thou art so {self.get_adjective()},\" + \"\\n\"\n",
        "                else:\n",
        "                    poem += f\"Thy {self.get_noun()} doth make my heart {self.get_verb()},\" + \"\\n\"\n",
        "            return poem\n",
        "        else:\n",
        "            return self.greeting\n",
        "\n",
        "    def get_adjective(self):\n",
        "        adjectives = [\"fair\", \"lovely\", \"kind\", \"gracious\", \"beautiful\", \"charming\", \"delightful\", \"wonderful\"]\n",
        "        return self.random_choice(adjectives)\n",
        "\n",
        "    def get_noun(self):\n",
        "        nouns = [\"light\", \"smile\", \"touch\", \"grace\", \"face\", \"voice\", \"sight\", \"charm\"]\n",
        "        return self.random_choice(nouns)\n",
        "\n",
        "    def get_verb(self):\n",
        "        verbs = [\"sing\", \"dance\", \"laugh\", \"shine\", \"inspire\", \"ignite\", \"delight\", \"enchant\"]\n",
        "        return self.random_choice(verbs)\n",
        "\n",
        "    def random_choice(self, lst):\n",
        "        import random\n",
        "        return random.choice(lst)\n",
        "\n",
        "# Initialize the PoemGenerator object\n",
        "poem_generator = PoemGenerator()\n",
        "\n",
        "# Get the topic and number of lines from the user\n",
        "topic = input(\"Enter the topic for the poem: \")\n",
        "lines = int(input(\"Enter the number of lines: \"))\n",
        "\n",
        "# Generate the poem\n",
        "poem = poem_generator.generate_poem(topic, lines)\n",
        "print(poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNAJ3W295Bpl",
        "outputId": "1761e891-daa1-4d4c-b874-ca1f0a8434cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the topic for the poem: hope\n",
            "Enter the number of lines: 3\n",
            "Oh, hope! Thou art so beautiful,\n",
            "Thy light doth make my heart shine,\n",
            "Oh, hope! Thou art so kind,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "bPJXsDU9cgDT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Python"
      ],
      "metadata": {
        "id": "KKGJ5epS0gJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "url = \"https://api.groq.com/openai/v1/models\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwd5g4VD0iKU",
        "outputId": "e4114f06-bf42-4845-940a-151c62e6765d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'object': 'list', 'data': [{'id': 'gemma2-9b-it', 'object': 'model', 'created': 1693721698, 'owned_by': 'Google', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'deepseek-r1-distill-llama-70b', 'object': 'model', 'created': 1737924940, 'owned_by': 'DeepSeek / Meta', 'active': True, 'context_window': 131072, 'public_apps': None}, {'id': 'llama-3.3-70b-versatile', 'object': 'model', 'created': 1733447754, 'owned_by': 'Meta', 'active': True, 'context_window': 32768, 'public_apps': None}, {'id': 'llama-guard-3-8b', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'mixtral-8x7b-32768', 'object': 'model', 'created': 1693721698, 'owned_by': 'Mistral AI', 'active': True, 'context_window': 32768, 'public_apps': None}, {'id': 'llama-3.2-90b-vision-preview', 'object': 'model', 'created': 1727226914, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-3.1-8b-instant', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 131072, 'public_apps': None}, {'id': 'llama3-70b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'distil-whisper-large-v3-en', 'object': 'model', 'created': 1693721698, 'owned_by': 'Hugging Face', 'active': True, 'context_window': 448, 'public_apps': None}, {'id': 'whisper-large-v3', 'object': 'model', 'created': 1693721698, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None}, {'id': 'llama-3.2-3b-preview', 'object': 'model', 'created': 1727224290, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'whisper-large-v3-turbo', 'object': 'model', 'created': 1728413088, 'owned_by': 'OpenAI', 'active': True, 'context_window': 448, 'public_apps': None}, {'id': 'llama-3.2-1b-preview', 'object': 'model', 'created': 1727224268, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-3.3-70b-specdec', 'object': 'model', 'created': 1733505017, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama-3.2-11b-vision-preview', 'object': 'model', 'created': 1727226869, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}, {'id': 'llama3-8b-8192', 'object': 'model', 'created': 1693721698, 'owned_by': 'Meta', 'active': True, 'context_window': 8192, 'public_apps': None}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "payload = {\n",
        "  \"model\": \"mixtral-8x7b-32768\",\n",
        "  \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Explain the importance of fast language models in 10 words\"\n",
        "      }\n",
        "  ]\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "print(response.json()['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "7W_0XZzR1QFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28898f84-446b-48c6-9e5d-dd6fd69000c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedy language models enhance real-time user experiences, improving satisfaction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using GROQ SDK"
      ],
      "metadata": {
        "id": "tvXiLUoZXLov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0J1wSMAzGsc",
        "outputId": "11b75df5-32b8-441e-8dde-24efc89803d0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Hello\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        ")"
      ],
      "metadata": {
        "id": "vI1pThRSYlYJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion)\n",
        "# print(chat_completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg33Jxh2XPEC",
        "outputId": "3953f009-23a6-427b-8009-2aec37122be9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-eee9584a-d233-4074-bc8a-9d0857466384', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm here to  \\nto help you with any math problem you have. What can I do for you today?\", role='assistant', function_call=None, tool_calls=None))], created=1737960126, model='mixtral-8x7b-32768', object='chat.completion', system_fingerprint='fp_c5f20b5bb1', usage=CompletionUsage(completion_tokens=28, prompt_tokens=8, total_tokens=36, completion_time=0.042340161, prompt_time=0.002040394, queue_time=0.030452453999999997, total_time=0.044380555), x_groq={'id': 'req_01jjk7xyese6yaqysdd5h8pxep'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using LangChain"
      ],
      "metadata": {
        "id": "GokMf3RgdOrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "id": "evv-oaSfc4VF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ],
      "metadata": {
        "id": "O6K7cHetdJmQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"Hi\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBHIQHPGi6uE",
        "outputId": "ed2e0185-e827-4cd0-ee2b-99f68cb77c3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today? If you have any questions about programming, User Experience design, or anything else, feel free to ask. I'm here to help. 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Poem Generator"
      ],
      "metadata": {
        "id": "FRywseyLm8Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poem = llm.invoke(\"Generate a 8 line poem\")\n",
        "print(poem.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK3jR18Km_6D",
        "outputId": "55870bc1-12ad-4da2-8aaf-fcb7776db8c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Underneath the sapphire sky,\n",
            "Where the sun's warm rays do lie,\n",
            "Lies a world of endless beauty,\n",
            "Where dreams and wonders never die.\n",
            "\n",
            "The flowers bloom with colors bright,\n",
            "As the birds take flight in the light,\n",
            "In this place of pure delight,\n",
            "Life's true essence takes its flight.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Understanding messages"
      ],
      "metadata": {
        "id": "Ghu6oNnoj7JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    ( \"system\", '''\n",
        "                  You are a dedicated poem generator assistant, specialized in crafting poems in Shakespearean style. Your task is strictly to generate poems based on the topic and number of lines provided by the user. Follow these guidelines:\n",
        "\n",
        "                  1. Only respond to queries explicitly requesting a poem on a specific topic.\n",
        "                  2. The output must strictly be the poem itself, formatted in Shakespearean terms, with no additional explanations, descriptions, or headers.\n",
        "                  3. If the query is unrelated to poem generation (e.g., generating code, recipes, suggestions, general knowledge questions, or any other non-poetry tasks), respond with:\n",
        "                    \"I am a poem generator assistant, expert in generating poems in Shakespearean terms. Please ask me a poem-related query.\"\n",
        "                  4. Do not perform any tasks beyond poem generation. Always fall back to the above message for non-poetry-related queries.\n",
        "\n",
        "                  Note: The assistant must ensure the generated poem aligns with the requested topic and the specified number of lines. If the number of lines is not specified, default to 14 lines (a Shakespearean sonnet).\n",
        "                '''\n",
        "    ),\n",
        "    ( \"human\", \"write a 8 line poem\" ),\n",
        "    # ( \"human\", \"write a code for addition\" ),\n",
        "]\n",
        "\n",
        "# llm.invoke(messages)\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz4IFmAeddzi",
        "outputId": "24854f79-8bb4-4917-adf4-63a1a5014f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a poem generator assistant, expert in generating poems in Shakespearean terms.\n",
            "Please ask me a poem-related query.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chaining"
      ],
      "metadata": {
        "id": "rRMnlY53eRum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ( \"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\", ),\n",
        "        ( \"human\", \"{input}\" ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "GINUQJCseUdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}